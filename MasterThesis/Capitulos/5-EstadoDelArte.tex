%----------------------------------------------------------------------------------------
%	ESTADO DEL ARTE (20-25 hojas)
%----------------------------------------------------------------------------------------

\pagestyle{empty}
\chapter {Estado del Arte}

La acepción de realidad virtual que va a ser tratado en este trabajo es un mundo generado por ordenador. Se puede interactuar con este mundo por medio diferentes dispositivos como los hápticos o los olfativos, pero la forma más común actualmente y en la cual se va a centrar este documento es en la representación de imágenes mediante un casco o gafas de realidad virtual.

\section{Marco teórico}

\subsection{Fotografía y vídeo}
La fotografía y el vídeo en realidad virtual es un campo muy amplio en el que se pueden encontrar diferentes maneras guardar y reproducir la información. A continuación se comentan algunos conceptos importantes para entender el resto del documento. 

\textbf{Nota:} Están organizados por conceptos alternativos, es decir, una foto no puede ser estereoscópica y monoscópica al mismo tiempo.

\subsubsection{Estereoscopía/Monoscopía}
La monoscopía implica que sólo existe una imagen para ambos ojos y por lo tanto no hay sensación de profundidad.

La estereoscopía es un factor muy importante en contenido multimedia para realidad virtual puesto que es el que más favorece la inmersión, como se explica en \cite{DiegoBezStereoscopy}. Consiste en tener dos imágenes que muestran la misma escena desde dos puntos cercanos a una distancia fija (normalmente de 6,5cm) y que representan la posición de los ojos. Cada una de estas imágenes se reproducen en una de las pantallas de las gafas, de tal manera que el cerebro del usuario se encarga de reconstruir la escena como si fuera realmente tridimensional. Aunque exista la ilusión de tridimensionalidad no es posible desplazarse por el entorno capturado.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{stereo-example}
  \caption{Ejemplo de imagen estéreo con lineas verticales para poder comparar el desplazamiento.}
  \label{fig:stereo-example}
\end{figure}

Es importante tener en cuenta que la separación entre las cámaras definirá la sensación de tamaño. Nuestro cerebro interpreta que si las cámaras están a menos distancia que nuestros ojos, todo es más grande y será mas pequeño si ponemos demasiada distancia.


\subsubsection{Campo de visión}
Por lo general se utilizan dos ángulos para el campo de visión. 

El más conocido y del que se suele hablar trata 360º de visión, es decir, una foto que genera una esfera alrededor de la vista del usuario. Es el que proporciona más inmersión.

El otro ángulo más utilizado es 180º que proporciona toda la visión frontal pero no hay imagen detrás. Las ventajas que más destacan son que puede ser grabado con cámaras poco especializadas y que se puede concentrar mayor cantidad de píxeles en el frente, consiguiendo mayor calidad en la acción destacada.

Existen diferentes métodos para la captura de vídeo como se explicará más adelante.

\subsubsection{Proyecciones}
Una proyección es una forma de representación de un elemento tridimensional, en este caso una geometría esférica, en un espacio bidimensional. A la hora de guardar o reproducir un contenido multimedia es importante tener en cuenta la proyección a utilizar ya que pueden distorisionar la imagen y saturar regiones poco interesantes, como veremos a continuación, con una gran cantidad de píxeles. 

Una de las proyecciones más extendidas y utilizadas en realidad virtual es la equirectangular (\ref{fig:equirect360-example} y \ref{fig:equirect360-schema}), que coincide con la proyección más utilizada en la actualidad para representar el mundo en dos dimensiones. Esta proyección proporciona demasiada información en los polos y que típicamente es el lugar al que menos se suele mirar. Esto hace que gran parte de los píxeles se malgasten. 

Facebook en \cite{FBDynamicStreamming}  propone varias mejoras para su sistema de streamming de vídeo en realidad virtual 360º. Algunas de estas mejoras pueden ser utilizadas también en vídeo local como por ejemplo aplicar una distorsión intencionada a la imagen proporcionando más espacio a la parte central de la imagen y compensando esa distorsión en el reproductor y así obtener más definición en las zonas importantes.

\begin{figure}[h]
\centering
\begin{subfigure}{.8\linewidth}
	\centering
  \includegraphics[width=\textwidth]{equirect360}
  \caption{Ejemplo de una proyección equirectangular.}
  \label{fig:equirect360-example}
\end{subfigure}\\
\begin{subfigure}{.8\linewidth}
	\centering
  \includegraphics[width=\textwidth]{equirect360-schema}
  \caption{Esquema de una proyección equirectangular.}
  \label{fig:equirect360-schema}
\end{subfigure}
\end{figure}

Otra proyección que se utiliza menos es la cúbica que divide la imagen en seis partes que forman las caras de un cubo. Tiene mayor densidad en las aristas del cubo pero el porcentaje de píxeles útiles aumenta. Normalmente se le aplica una deformación al cubo dándole curvatura reduciendo la densidad de píxeles para que la distribución se más uniforme.

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{cubemap}
  \caption{Ejemplo de proyección cúbica.}
  \label{fig:cubemap-example}
\end{figure}

Por último mencionar la proyección de barril que se construye como un cilindro. La distribución de píxeles es uniforme en el campo de visión típico. Esta proyección desaprovecha píxeles que se pierden en los huecos que dejan las tapas del cilindro.

Existen una infinidad de proyecciones y cada una tiene una serie de ventajas e inconvenientes.
 
\subsubsection{Omni-directional Stereo}
La captura con cámara de imágenes estereoscópicas 360º no es trivial.. Existen varios métodos para conseguir esto, y en vídeo el más común consiste en capturar un fotograma en diferentes puntos de una circunferencia teniendo en cuenta el radio de la cabeza, después esas fotos se unen y se reconstruye la imágen. Este proceso es complicado y si no se hace bien, se produce un efecto en el ensambrado llamado stitching y que se evidencia con unos saltos de color o cortes en objetos. Este problema no es trivial y cuesta eliminarlo como se comenta en \cite{DiegoBezStitching}.

Disney y Google proponen un sistema para captura de imágenes llamado ODS (Omni-directional stereo) que explican detalladamente en \cite{GoogleStereoscopy} y \cite{DisneyOmnistereoscopic}. Este sistema consiste en dos cámaras, posicionadas a la distancia ocular y puestas en una circunferencia de un radio similar al de una cabeza. Durante la captura, las cámaras dan una vuelta por la circunferencia y guarda una linea de píxeles por cada posición de la cámara. Esto tiene como consecuencia que únicamente exista una linea de píxeles correcta en cada posición como se puede ver en la figura \ref{fig:ods-schema}.  conlleva implicaciones como que los objetos en la visión periférica no se visualizan correctamente. Pese a esto, la sensación que se consigue es buena y proporciona una gran inmersión.

\begin{figure}[h]
\centering
\begin{subfigure}{.47\linewidth}
	\centering
  \includegraphics[width=\textwidth]{correct-vision}
  \caption{Esquema de la visión completa.}
  \label{fig:correct-vision-schema}
\end{subfigure}%
\hspace{.05\linewidth}
\begin{subfigure}{.47\linewidth}
	\centering
  \includegraphics[width=\textwidth]{ods}
  \caption{Esquema la aproximación hecha con ODS.}
  \label{fig:ods-schema}
\end{subfigure}
\caption{Imágenes obtenidas de \cite{GoogleStereoscopy}.}
\end{figure}

Por último comentar que el vídeo 180º tiene como beneficio que no necesita de ningún sistema como ODS ya que se hace directamente con cámara sin recurrir a métodos adicionales. Esto hace que todo el area de visualización sea correcta como se muestra en la figura \ref{fig:correct-vision-schema}.

\subsection{Mapas de profundidad}
Debido a la cantidad de técnicas que utilizan mapas de profundidad o \textit{depthmap} es interesante explicar en que consisten. 

Los mapas de profundidad son imagenes que en cada pixel se encuentra codificada la profundidad de la foto en ese punto. Generalmente se utiliza una escala de grises o de rojos aunque se pueden recurrir a métodos más complejos.\cite{Josh6DoFUnity}

\begin{figure}[h]
  \centering
  \includegraphics[width=\textwidth]{red-depthmap-example}
  \caption{Ejemplo de mapa de profundidad 360 en escala de rojos}
  \label{fig:red-depthmap-example}
\end{figure}

En el caso de la escala de grises (\ref{fig:depthmap-example}), los tonos más oscuros representan elementos en el fondo de la imagen, mientras que los tonos mas claro representan elementos más cercanos.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.7\textwidth]{depthmap-example}
  \caption{Ejemplo de mapa de profundidad cenital en escala de grises.}
  \label{fig:depthmap-example}
\end{figure}

En el caso de una imagen generada por ordenador, es fácil obtener un buen mapa de profundidad. Sin embargo en el caso de las imágenes captadas por cámaras reales, existe la posibilidad de que la cámara esté preparada o en caso contrario habría que aplicar algoritmos que calculen la profundidad en cada píxel. 

Las cámaras que están preparadas para obtener el mapa de profundidad utilizan típicamente la emisión de infrarrojos. Existe una tecnología llamada LIDAR que obtiene mapas de profundidad de alta precisión con un haz láser, pero el tiempo que tarda en obtenerlo no lo hace compatible con la grabación de vídeo.

Dentro de los algoritmos que infieren el mapa de profundidad, los más conocidos utilizan imágenes estereoscópicas como el algoritmo BM  intenta parear elementos que se encuentren a la misma altura y el algoritmo SGBM que es una variación del anterior añadiendo una ventana de búsqueda para encontrar las correspondencias.

\subsection{Fotogrametría}
La fotogrametría consiste en deducir la ubicación de múltiples puntos en el espacio a partir de una serie de fotografías. Después de eso se reconstruyen los triángulos para generar una malla texturizada que represente de la manera más fiel posible la escena fotografiada. Javier de Matías analiza en profundidad la fotogrametría en \cite{PhotogrametryThesis}.

Al igual que en los mapas de profundidad, utilizar imágenes estereoscópicas ayudan a reconstruir la malla con mayor facilidad.

Algunos de los programas más conocidos para generación de mallas a partir de fotos son PIX4D y PhotoScan entre otros.


\section{Project Sidewinder}
Adobe presento en 2017 \cite{SidewinderAdobe} una demo que utilizaba un \textit{depthmap} de manera muy sencilla para permitir seis grados de libertad dentro de un video 360º. No proporcionan mucha información ya que es una prueba de concepto.

El desplazamiento punto a punto parece correcto pero se ve una distorsión en los bordes probablemente debido al estado temprano del proyecto.

\section{Nube de puntos}
Josh en \cite{Josh6DoFUnity} nos muestra una aplicación de esta técnica creando un punto en el espacio por cada píxel de la foto o el vídeo donde indique el mapa de profundidad.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\textwidth]{pointcloud-example}
  \caption{Ejemplo de nube de puntos obtenida de \cite{Josh6DoFUnity}.}
  \label{fig:pointcloud-example}
\end{figure}

Esta implementación por contra provoca que aparezcan muchos huecos como se puede ver en \ref{fig:depthmap-example} y generalmente se suele acompañar la implementación con una selección del tamaño del punto como muestra \cite{Josh6DoFUnity} para ver una imagen más sólida.

Otro de los problemas que tiene esta técnica es la cantidad de puntos que deben ser tratados, ya que una resolución \textit{QHD} (2560x1440) requiere cuatro millones de puntos siendo la resolución recomendada actualmente es \textit{4K}. Probablemente el rendimiento sea bajo en equipos poco potentes y en dispositivos móviles.

\section{Cámara y herramientas de realidad virtual de OTOY y Facebook}

En el F8 de 2017 en Los Ángeles \textit{OTOY} y \textit{Facebook} presentaron una colaboración para producir vídeo 360º volumétrico e interactivo a un precio asequible \cite{OtoyVR}. No hay noticias de 2018 por lo que puede que esté abandonado.

La colaboración consistía en una cámara 360º especializada y una serie de herramientas para procesar el contenido y visualizarlo. El procedimiento implica subir el contenido a la nube de OTOY para procesarlo y así reconstruir la escena como una malla tridimensional.

La calidad presentada en las demos era buena con poca distorsión aunque el desplazamiento que presentaban era pequeño.

\section{Facebook: Seis Grados de libertad con Fotogrametría}
Una de las formas de conseguir seis grados de libertad es reconstruir la escena mediante fotogrametría como muestra \textit{Facebook} en \cite{FBCasual3DCapture} para procesar la imagen y obtener una malla que pueda ser mostrada usando técnicas convencionales.

\begin{figure}[h]
  \centering
	\includegraphics[width=\linewidth]{FB/fbdepthmap}
  \caption{Conferencia mostrando un mapa de profundidad de limite inferior obtenida de \cite{FBCasual3DCapture}.}
  \label{fig:fbLBdepthmap-example}
\end{figure}

Este procedimiento utiliza una técnica nueva que han llamado ``mapa de profundidad de limite inferior'' (\ref{fig:depthmap-example}) que la profundidad de cada pixel debe ser estrictamente mayor que en el depthmap normal. Crear este mapa de profundidad le ayuda al ensamblado de las imágenes para crear la imágen final 360º. Además mezclando el mapa de profundidad con otros algoritmos son capaces de obtener un mapa de normales bastante preciso. \ref{fig:fbnormalmapcorrected-example}

\begin{figure}[h]
\centering
\begin{subfigure}{.47\linewidth}
	\centering
	\includegraphics[width=\linewidth]{FB/fbnormalartifacts}
  \caption{Conferencia mostrando los artefactos que obtienen.}
  \label{fig:fbnormalmapartifacts-example}
\end{subfigure}%
\hspace{.05\linewidth}
\begin{subfigure}{.47\linewidth}
	\centering
	\includegraphics[width=\linewidth]{FB/fbnormalmap}
  \caption{Conferencia mostrando los artefactos arreglados.}
  \label{fig:fbnormalmapcorrected-example}
\end{subfigure}
\caption{Imágenes obtenidas de \cite{FBCasual3DCapture}.}
\end{figure}


Finalmente crean la malla a partir de una nube de puntos. Esta malla como veremos en otras técnicas, tiene agujeros detrás de los objetos que no pueden ser rellenados por falta de información. En este caso optan por difuminar de manera sutil las zonas desconocidas dando un buen resultado.

Todo esto lo aprovechan para poder generar un entorno tridimensional con el que poder interactuar y lo ejemplifican jugando con la iluminación \ref{fig:fblighting-example} o incluso inundando la escena \ref{fig:fbflooding-example}.

\begin{figure}[h]
\centering
\begin{subfigure}{.47\linewidth}
	\centering
	\includegraphics[width=\linewidth]{FB/fblighting}
  \caption{Conferencia mostrando la malla iluminada.}
  \label{fig:fblighting-example}
\end{subfigure}%
\hspace{.05\linewidth}
\begin{subfigure}{.47\linewidth}
	\centering
	\includegraphics[width=\linewidth]{FB/fbflooding}
  \caption{Conferencia mostrando un escenario inundándose.}
  \label{fig:fbflooding-example}
\end{subfigure}
\caption{Imágenes obtenidas de \cite{FBCasual3DCapture}.}
\end{figure}

Una de las limitaciones que tiene este método es que está diseñado para fotografía en 360º y no se menciona en ningún momento al vídeo 360º por lo que se puede deducir que no está preparado. Por otro lado este tipo de procesado de imágenes requiere una cantidad grande de tiempo.

\section{``Welcome to Lightfields''}
Una compañía llamada Lytro creo un sistema que llamo campos de luz o Lightfields \cite{LytroLightfields} por su similitud con el concepto físico. Más tarde Google se interesó por la compañía comprando algunas de sus patentes e incorporando empleados a su plantilla.

\textit{Google} continuó el proyecto \cite{GoogleLightfields} y construyó un soporte que permite hacer fotografías de una escena desde puntos situados en una esfera.

\begin{figure}[h]
  \centering
	\includegraphics[width=\linewidth]{lightfields-camera}
  \caption{Prototipo haciendo una captura del interior de una cabina. Obtenida de \cite{GoogleLightfields}.}
  \label{fig:lightfields-camera}
\end{figure}

Este sistema no comprime las imágenes, sino que las utiliza directamente en el programa por lo que una ``foto'' en lightfields ocupa más de 250MB. A partir de todas las fotos se calcula la imagen correspondiente en tiempo real en función de la posición del usuario haciendo una interpolación entre diferentes imágenes, eso hace que se recupere una imagen muy fiel a lo que se vería en la realidad.

Como se puede apreciar en \ref{fig:lightfields-schema}, el usuario debe estar dentro de la esfera de fotos, por lo que el rango de movimiento está limitado a menos de un metro.

\begin{figure}[h]
  \centering
	\includegraphics[width=0.5\linewidth]{lightfields-schema}
  \caption{Esquema de funcionamiento de los lightfields. Obtenida de \cite{GoogleLightfields}.}
  \label{fig:lightfields-schema}
\end{figure}

Además este sistema tiene una implicación que no había sido contemplada hasta ahora que son las superficies especulares. Estas superficies en el resto de técnicas se podían ver con reflejo o no, pero nunca respondería a la posición del usuario. La interpolación de imágenes hace que los espejos reflejen algo diferente en cada posición de la cabeza.

Esta técnica para fotografía es probablemente la mejor en cuanto a calidad de imagen pero sin embargo no se puede utilizar en vídeo debido al tiempo que se tarda en capturar un sólo fotograma. Tampoco permite alterar la luz como se mostraba en la demo de \textit{Facebook}.











